{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('./goemotions/train.tsv', sep='\\t', header=None)\n",
    "val_data = pd.read_csv('./goemotions/dev.tsv', sep='\\t', header=None)\n",
    "test_data = pd.read_csv('./goemotions/test.tsv', sep='\\t', header=None)\n",
    "\n",
    "train_data.columns = ['text', 'label', 'code']\n",
    "val_data.columns = ['text', 'label', 'code']\n",
    "test_data.columns = ['text', 'label', 'code']\n",
    "\n",
    "# Basic preprocessing\n",
    "def preprocess_text(df):\n",
    "    df['cleaned_text'] = df['text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    return df\n",
    "\n",
    "train_data = preprocess_text(train_data)\n",
    "val_data = preprocess_text(val_data)\n",
    "test_data = preprocess_text(test_data)\n",
    "\n",
    "# Split the 'label' column into a list of labels (comma-separated integers)\n",
    "def split_labels(label):\n",
    "    # Convert the label string (e.g., \"6,22\") into a list of integers [6, 22]\n",
    "    return list(map(int, label.split(',')))\n",
    "\n",
    "train_data['label'] = train_data['label'].apply(split_labels)\n",
    "val_data['label'] = val_data['label'].apply(split_labels)\n",
    "test_data['label'] = test_data['label'].apply(split_labels)\n",
    "\n",
    "# Use MultiLabelBinarizer to encode the labels into binary format\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(train_data['label'])\n",
    "y_test = mlb.fit_transform(test_data['label'])\n",
    "y_val = mlb.transform(val_data['label'])\n",
    "\n",
    "X_train = train_data['cleaned_text']\n",
    "X_test = test_data['cleaned_text']\n",
    "X_val = val_data['cleaned_text']\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Define classifiers with OneVsRestClassifier for multilabel classification\n",
    "classifiers = {\n",
    "    'Logistic Regression': OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "    'Support Vector Classifier': OneVsRestClassifier(SVC(probability=True)),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),  # No need for OneVsRest\n",
    "    'Random Forest': OneVsRestClassifier(RandomForestClassifier())\n",
    "}\n",
    "\n",
    "# # Train and evaluate each classifier on the validation set\n",
    "# for name, classifier in classifiers.items():\n",
    "#     # Create pipeline with TF-IDF and classifier\n",
    "#     model = Pipeline([\n",
    "#         ('tfidf', tfidf),\n",
    "#         ('clf', classifier)\n",
    "#     ])\n",
    "    \n",
    "#     # Train the model\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on validation data\n",
    "#     if name == 'Support Vector Classifier':\n",
    "#         y_val_pred_proba = model.decision_function(X_val)\n",
    "#         y_val_pred = np.where(y_val_pred_proba > 0, 1, 0)  \n",
    "#     else:\n",
    "#         # For other classifiers, use predict directly\n",
    "#         y_val_pred = model.predict(X_val)\n",
    "    \n",
    "#     # Evaluate the model on validation data\n",
    "#     print(f\"Results for {name}:\")\n",
    "#     print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(y_val, y_val_pred, target_names=mlb.classes_))\n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "## tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = 100\n",
    "epochs_num = 2\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Define the LSTM model for multilabel classification\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=maxlen),\n",
    "    LSTM(units=128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=len(mlb.classes_), activation='sigmoid')  # Use sigmoid for multilabel classification\n",
    "])\n",
    "\n",
    "# Compile the model using binary_crossentropy loss for multilabel classification\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_pad, y_train, epochs=epochs_num, batch_size=64, validation_data=(X_val_pad, y_val))\n",
    "\n",
    "# Predict on validation data\n",
    "lstm_val_pred = lstm_model.predict(X_val_pad)\n",
    "\n",
    "# Convert probabilities to binary labels (using a threshold of 0.5)\n",
    "lstm_val_pred_classes = (lstm_val_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model using classification_report\n",
    "print(\"LSTM:\\n\", classification_report(y_val, lstm_val_pred_classes, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# List physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available physical devices:\", physical_devices)\n",
    "\n",
    "# Check if TensorFlow is using the GPU\n",
    "print(\"Is TensorFlow using GPU?\", tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "# Number of labels (using len(mlb.classes_) from MultiLabelBinarizer)\n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(data, max_length=128):\n",
    "    return tokenizer(\n",
    "        data['text'].tolist(),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "X_train = tokenize_data(train_data)\n",
    "X_val = tokenize_data(val_data)\n",
    "X_test = tokenize_data(test_data)\n",
    "batch_size = 32\n",
    "\n",
    "# Prepare TensorFlow dataset objects for multilabel classification\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': X_train['input_ids'], \n",
    "        'attention_mask': X_train['attention_mask'], \n",
    "        'token_type_ids': X_train['token_type_ids']\n",
    "    },\n",
    "    y_train  # Use multilabel binarized labels here (already binarized using MultiLabelBinarizer)\n",
    ")).batch(batch_size).shuffle(1000)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': X_val['input_ids'], \n",
    "        'attention_mask': X_val['attention_mask'], \n",
    "        'token_type_ids': X_val['token_type_ids']\n",
    "    },\n",
    "    y_val  # Use multilabel binarized labels here\n",
    ")).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': X_test['input_ids'], \n",
    "        'attention_mask': X_test['attention_mask'], \n",
    "        'token_type_ids': X_test['token_type_ids']\n",
    "    },\n",
    "    y_test  # Use multilabel binarized labels here\n",
    ")).batch(batch_size)\n",
    "\n",
    "# Compile the model for multilabel classification\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),\n",
    "    loss=BinaryCrossentropy(from_logits=True),  # Use binary cross-entropy for multilabel classification\n",
    "    metrics=[AUC(name='auc')]  # Use AUC as a metric for multilabel classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_auc = model.evaluate(test_dataset)\n",
    "print(f'Test AUC: {test_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have collected the classification reports\n",
    "results = {\n",
    "    'Logistic Regression': lr_val_pred,\n",
    "    'SVM': svm_val_pred,\n",
    "    'Naive Bayes': nb_val_pred,\n",
    "    'Random Forest': rf_val_pred,\n",
    "    'LSTM': lstm_val_pred_classes,\n",
    "    # 'BERT': val_pred_classes\n",
    "    # Add GRU, CNN, and BERT results\n",
    "}\n",
    "\n",
    "# Compare accuracy, precision, recall, and F1-score\n",
    "for model, preds in results.items():\n",
    "    print(f\"{model}:\\n\", classification_report(y_val, preds))\n",
    "    \n",
    "metrics = {}\n",
    "\n",
    "for model, preds in results.items():\n",
    "    report = classification_report(y_val, preds, output_dict=True)\n",
    "    metrics[model] = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'macro avg': {\n",
    "            'precision': report['macro avg']['precision'],\n",
    "            'recall': report['macro avg']['recall'],\n",
    "            'f1-score': report['macro avg']['f1-score']\n",
    "        },\n",
    "        'weighted avg': {\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'f1-score': report['weighted avg']['f1-score']\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Print collected metrics\n",
    "for model, stats in metrics.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Accuracy: {stats['accuracy']:.4f}\")\n",
    "    print(f\"  Macro Average Precision: {stats['macro avg']['precision']:.4f}\")\n",
    "    print(f\"  Macro Average Recall: {stats['macro avg']['recall']:.4f}\")\n",
    "    print(f\"  Macro Average F1-score: {stats['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"  Weighted Average Precision: {stats['weighted avg']['precision']:.4f}\")\n",
    "    print(f\"  Weighted Average Recall: {stats['weighted avg']['recall']:.4f}\")\n",
    "    print(f\"  Weighted Average F1-score: {stats['weighted avg']['f1-score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model = lstm_model  # Assuming LSTM is the best model\n",
    "test_pred = best_model.predict(X_test_pad)\n",
    "test_pred_classes = np.argmax(test_pred, axis=1)\n",
    "print(\"Final Model Evaluation on Test Set:\\n\", classification_report(y_test, test_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# List of models and their predicted labels\n",
    "models_predictions = {\n",
    "    'Logistic Regression': lr_val_pred,\n",
    "    'SVM': svm_val_pred,\n",
    "    'Naive Bayes': nb_val_pred,\n",
    "    'Random Forest': rf_val_pred\n",
    "}\n",
    "\n",
    "# Function to extract metrics\n",
    "def extract_metrics(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    precision = np.mean([report[label]['precision'] for label in report if label.isdigit()])\n",
    "    recall = np.mean([report[label]['recall'] for label in report if label.isdigit()])\n",
    "    f1_score = np.mean([report[label]['f1-score'] for label in report if label.isdigit()])\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "# Initialize lists for metrics\n",
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for model_name, y_pred in models_predictions.items():\n",
    "    accuracy, precision, recall, f1_score = extract_metrics(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "    print(f\"{model_name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1_score}\")\n",
    "    \n",
    "# Plotting the metrics\n",
    "def plot_metric(models, metric_values, metric_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=metric_values)\n",
    "    plt.title(f'{metric_name} Comparison')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "models = list(models_predictions.keys())\n",
    "\n",
    "plot_metric(models, accuracies, 'Accuracy')\n",
    "plot_metric(models, precisions, 'Precision')\n",
    "plot_metric(models, recalls, 'Recall')\n",
    "plot_metric(models, f1_scores, 'F1 Score')\n",
    "\n",
    "# Plot confusion matrix for each model\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "for model_name, y_pred in models_predictions.items():\n",
    "    plot_confusion_matrix(y_test, y_pred, model_name)\n",
    "    \n",
    "# Assuming 'y_train' and 'y_test' are encoded labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_train)\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Załóżmy, że y_true to rzeczywiste etykiety, a y_pred to przewidywane etykiety\n",
    "# y_true = ...  # rzeczywiste etykiety\n",
    "# y_pred = ...  # przewidywane etykiety\n",
    "\n",
    "# Generowanie macierzy pomyłek\n",
    "cm = confusion_matrix(y_test, lstm_val_pred_classes)\n",
    "\n",
    "# Wizualizacja macierzy pomyłek\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zakładamy, że masz historyczny obiekt z trenowania modelu: history = model.fit(...)\n",
    "\n",
    "# Rysowanie wykresu dokładności\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Rysowanie wykresu strat\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
