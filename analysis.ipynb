{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('./training.csv')\n",
    "val_data = pd.read_csv('./validation.csv')\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "\n",
    "# Basic preprocessing\n",
    "def preprocess_text(df):\n",
    "    df['cleaned_text'] = df['text'].str.lower().str.replace(r'[^\\w\\s]', '')\n",
    "    return df\n",
    "\n",
    "train_data = preprocess_text(train_data)\n",
    "val_data = preprocess_text(val_data)\n",
    "test_data = preprocess_text(test_data)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data['label'])\n",
    "y_val = label_encoder.transform(val_data['label'])\n",
    "y_test = label_encoder.transform(test_data['label'])\n",
    "\n",
    "X_train = train_data['cleaned_text']\n",
    "X_val = val_data['cleaned_text']\n",
    "X_test = test_data['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "lr_val_pred = lr.predict(X_val_tfidf)\n",
    "print(\"Logistic Regression:\\n\", classification_report(y_val, lr_val_pred))\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "svm_val_pred = svm.predict(X_val_tfidf)\n",
    "print(\"SVM:\\n\", classification_report(y_val, svm_val_pred))\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "nb_val_pred = nb.predict(X_val_tfidf)\n",
    "print(\"Naive Bayes:\\n\", classification_report(y_val, nb_val_pred))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "rf_val_pred = rf.predict(X_val_tfidf)\n",
    "print(\"Random Forest:\\n\", classification_report(y_val, rf_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "## tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = 100\n",
    "epochs_num = 2\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=maxlen),\n",
    "    LSTM(units=128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_pad, y_train, epochs=epochs_num, batch_size=64, validation_data=(X_val_pad, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_val_pred = lstm_model.predict(X_val_pad)\n",
    "lstm_val_pred_classes = np.argmax(lstm_val_pred, axis=1)\n",
    "print(\"LSTM:\\n\", classification_report(y_val, lstm_val_pred_classes))\n",
    "\n",
    "# Define the GRU model\n",
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=maxlen),\n",
    "    GRU(units=128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "gru_model.fit(X_train_pad, y_train, epochs=epochs_num, batch_size=64, validation_data=(X_val_pad, y_val)) # change epochs\n",
    "\n",
    "# Evaluate the model\n",
    "gru_val_pred = gru_model.predict(X_val_pad)\n",
    "gru_val_pred_classes = np.argmax(gru_val_pred, axis=1)\n",
    "print(\"GRU:\\n\", classification_report(y_val, gru_val_pred_classes))\n",
    "\n",
    "# Define the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=maxlen),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(units=len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_pad, y_train, epochs=epochs_num, batch_size=64, validation_data=(X_val_pad, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "cnn_val_pred = cnn_model.predict(X_val_pad)\n",
    "cnn_val_pred_classes = np.argmax(cnn_val_pred, axis=1)\n",
    "print(\"CNN:\\n\", classification_report(y_val, cnn_val_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# List physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available physical devices:\", physical_devices)\n",
    "\n",
    "# Check if TensorFlow is using the GPU\n",
    "print(\"Is TensorFlow using GPU?\", tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "# import os as _os\n",
    "# _os.add_dll_directory(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.2\\\\bin\")\n",
    "\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#     physical_devices[0],\n",
    "#     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "num_labels = len(train_data['label'].unique())\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(data, max_length=128):\n",
    "    return tokenizer(\n",
    "        data['text'].tolist(),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "X_train = tokenize_data(train_data)\n",
    "X_val = tokenize_data(val_data)\n",
    "X_test = tokenize_data(test_data)\n",
    "batch_size = 32\n",
    "\n",
    "# Prepare TensorFlow dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': X_train['input_ids'], \n",
    "        'attention_mask': X_train['attention_mask'], \n",
    "        'token_type_ids': X_train['token_type_ids']\n",
    "    },\n",
    "    y_train\n",
    ")).batch(batch_size).shuffle(1000)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': X_val['input_ids'], \n",
    "        'attention_mask': X_val['attention_mask'], \n",
    "        'token_type_ids': X_val['token_type_ids']\n",
    "    },\n",
    "    y_val\n",
    ")).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': X_test['input_ids'], \n",
    "        'attention_mask': X_test['attention_mask'], \n",
    "        'token_type_ids': X_test['token_type_ids']\n",
    "    },\n",
    "    y_test\n",
    ")).batch(batch_size)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[SparseCategoricalAccuracy('accuracy')]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1\n",
    ")\n",
    "\n",
    "y_test_pred = model.predict(test_dataset)\n",
    "y_test_pred_prob = tf.sigmoid(y_test_pred.logits).numpy()  \n",
    "\n",
    "y_test_pred_classes = (y_test_pred_prob > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have collected the classification reports\n",
    "results = {\n",
    "    'Logistic Regression': lr_val_pred,\n",
    "    'SVM': svm_val_pred,\n",
    "    'Naive Bayes': nb_val_pred,\n",
    "    'Random Forest': rf_val_pred,\n",
    "    'LSTM': lstm_val_pred_classes,\n",
    "    'BERT': y_test_pred_classes,\n",
    "}\n",
    "\n",
    "# Compare accuracy, precision, recall, and F1-score\n",
    "for model, preds in results.items():\n",
    "    print(f\"{model}:\\n\", classification_report(y_val, preds))\n",
    "    \n",
    "metrics = {}\n",
    "\n",
    "test_loss, test_auc = model.evaluate(test_dataset)\n",
    "\n",
    "for model, preds in results.items():\n",
    "    report = classification_report(y_val, preds, output_dict=True)\n",
    "    metrics[model] = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'macro avg': {\n",
    "            'precision': report['macro avg']['precision'],\n",
    "            'recall': report['macro avg']['recall'],\n",
    "            'f1-score': report['macro avg']['f1-score']\n",
    "        },\n",
    "        'weighted avg': {\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'f1-score': report['weighted avg']['f1-score']\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Print collected metrics\n",
    "for model, stats in metrics.items():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Accuracy: {stats['accuracy']:.4f}\")\n",
    "    print(f\"  Macro Average Precision: {stats['macro avg']['precision']:.4f}\")\n",
    "    print(f\"  Macro Average Recall: {stats['macro avg']['recall']:.4f}\")\n",
    "    print(f\"  Macro Average F1-score: {stats['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"  Weighted Average Precision: {stats['weighted avg']['precision']:.4f}\")\n",
    "    print(f\"  Weighted Average Recall: {stats['weighted avg']['recall']:.4f}\")\n",
    "    print(f\"  Weighted Average F1-score: {stats['weighted avg']['f1-score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model = lstm_model  # Assuming LSTM is the best model\n",
    "test_pred = best_model.predict(X_test_pad)\n",
    "test_pred_classes = np.argmax(test_pred, axis=1)\n",
    "print(\"Final Model Evaluation on Test Set:\\n\", classification_report(y_test, test_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# List of models and their predicted labels\n",
    "models_predictions = {\n",
    "    'Logistic Regression': lr_val_pred,\n",
    "    'SVM': svm_val_pred,\n",
    "    'Naive Bayes': nb_val_pred,\n",
    "    'Random Forest': rf_val_pred\n",
    "}\n",
    "\n",
    "# Function to extract metrics\n",
    "def extract_metrics(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    precision = np.mean([report[label]['precision'] for label in report if label.isdigit()])\n",
    "    recall = np.mean([report[label]['recall'] for label in report if label.isdigit()])\n",
    "    f1_score = np.mean([report[label]['f1-score'] for label in report if label.isdigit()])\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "# Initialize lists for metrics\n",
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for model_name, y_pred in models_predictions.items():\n",
    "    accuracy, precision, recall, f1_score = extract_metrics(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "    print(f\"{model_name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1_score}\")\n",
    "    \n",
    "# Plotting the metrics\n",
    "def plot_metric(models, metric_values, metric_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=models, y=metric_values)\n",
    "    plt.title(f'{metric_name} Comparison')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "models = list(models_predictions.keys())\n",
    "\n",
    "plot_metric(models, accuracies, 'Accuracy')\n",
    "plot_metric(models, precisions, 'Precision')\n",
    "plot_metric(models, recalls, 'Recall')\n",
    "plot_metric(models, f1_scores, 'F1 Score')\n",
    "\n",
    "# Plot confusion matrix for each model\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "for model_name, y_pred in models_predictions.items():\n",
    "    plot_confusion_matrix(y_test, y_pred, model_name)\n",
    "    \n",
    "# Assuming 'y_train' and 'y_test' are encoded labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_train)\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Załóżmy, że y_true to rzeczywiste etykiety, a y_pred to przewidywane etykiety\n",
    "# y_true = ...  # rzeczywiste etykiety\n",
    "# y_pred = ...  # przewidywane etykiety\n",
    "\n",
    "# Generowanie macierzy pomyłek\n",
    "cm = confusion_matrix(y_test, lstm_val_pred_classes)\n",
    "\n",
    "# Wizualizacja macierzy pomyłek\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zakładamy, że masz historyczny obiekt z trenowania modelu: history = model.fit(...)\n",
    "\n",
    "# Rysowanie wykresu dokładności\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Rysowanie wykresu strat\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
